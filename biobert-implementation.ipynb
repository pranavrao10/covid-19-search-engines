{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65398c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4222b0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "171173\n",
      "{'doc_id': 'ug7v899j', 'cleaned_text': 'clinical features of culture-proven mycoplasma pneumoniae infections at king abdulaziz university hospital, jeddah, saudi arabia objective: this retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven mycoplasma pneumoniae infections at king abdulaziz university hospital, jeddah, saudi arabia. methods: patients with positive m. pneumoniae cultures from respiratory specimens from january 1997 through december 1998 were identified through the microbiology records. charts of patients were reviewed. results: 40 patients were identified, 33 (82.5%) of whom required admission. most infections (92.5%) were community-acquired. the infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). it occurred year-round but was most common in the fall (35%) and spring (30%). more than three-quarters of patients (77.5%) had comorbidities. twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, p = 0.05). of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of m pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. the 3 patients who died of m pneumoniae pneumonia had other comorbidities. conclusion: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'biobert_encoded_chunks': [{'input_ids': tensor([[  101,  7300,  1956,  1104,  2754,   118,  9893,  1139,   108,   108,\n",
      "          1884,   108,   108,   185,   108,   108, 17496,   108,   108, 12477,\n",
      "         20673,   108,   108,   174, 16565,  1120,  2226,   170,   108,   108,\n",
      "           171,   108,   108,  3840,   108,   108,  2495,   108,   108,   195,\n",
      "          1182,   108,   108,   195,  2755,  2704,   117,   179,   108,   108,\n",
      "          5048,  1181,   108,   108, 18257,   117, 21718,   108,   108,   190,\n",
      "          3309,   170,   108,   108,   187,  6639,   108,   108,   178,  1161,\n",
      "          7649,   131,  1142, 18675,  3481,  3189,  4856,  1103,   174,   108,\n",
      "           108,   185,  2386,   108,   108,  9712,   108,   108,   178,  4807,\n",
      "          1105,  7300,  1956,  1104,  1969,  4420,  1114,  2754,   118,  9893,\n",
      "          1139,   108,   108,  1884,   108,   108,   185,   108,   108, 17496,\n",
      "           108,   108, 12477, 20673,   108,   108,   174, 16565,  1120,  2226,\n",
      "           170,   108,   108,   171,   108,   108,  3840,   108,   108,  2495,\n",
      "           108,   108,   195,  1182,   108,   108,   195,  2755,  2704,   117,\n",
      "           179,   108,   108,  5048,  1181,   108,   108, 18257,   117, 21718,\n",
      "           108,   108,   190,  3309,   170,   108,   108,   187,  6639,   108,\n",
      "           108,   178,  1161,   119,  4069,   131,  4420,  1114,  3112,   182,\n",
      "           119, 20673,   108,   108,   174,  8708,  1121, 19192,  9985,  1121,\n",
      "           179,   108,   108,  1126,  1358,   108,   108,   170,  1616,  1816,\n",
      "          1194,  1260,   108,   108,   172,  1162,   108,   108,   182,  3169,\n",
      "          1772,  1127,  3626,  1194,  1103, 17599,   108,   108, 10256,  3002,\n",
      "           119,  5896,  1104,  4420,  1127,  7815,   119,  2686,   131,  1969,\n",
      "          4420,  1127,  3626,   117,  3081,   113,  5787,   119,   126,   110,\n",
      "           114,  1104,  2292,  2320, 10296,   119,  1211, 16565,   113,  5556,\n",
      "           119,   126,   110,   114,  1127,  1661,   118,  2888,   119,  1103,\n",
      "          8974,  4634,  1155,  1425,  2114,  1133,  1108,  1211,  1887,  1107,\n",
      "         20186,   113,  2724,   119,   126,   110,   114,  1105,  3073,   118,\n",
      "          1278,  1482,   113,  1659,   119,   126,   110,   114,   119,  1122,\n",
      "          3296,  1214,   118,  1668,  1133,  1108,  1211,  1887,  1107,  1103,\n",
      "          2303,   113,  2588,   110,   114,  1105,  3450,   113,  1476,   110,\n",
      "           114,   119,  1167,  1190,  1210,   118,  7541,  1104,  4420,   113,\n",
      "          5581,   119,   126,   110,   114,  1125,  3254,   108,   108,  1137,\n",
      "           108,   108,  6875,   108,   108,  1122,  1905,   119,  2570,   118,\n",
      "          1300,  1110,   108,   108,   184,  8052,   108,   108,   188,   113,\n",
      "          2539,   110,   114,  1127,  2628,  1114, 20673,   117,  1489,   113,\n",
      "          2588,   110,   114,  1114,  3105, 19192, 14441, 16565,   117,  1105,\n",
      "           123,   113,   126,   110,   114,  1114,  9304,   108,   108,  1113,\n",
      "           108,   108, 22572,  1182,   108,   108,   184,  2646,   108,   108,\n",
      "           189,  1548,   119, 21810,   113,  5787,   119,   126,   110,   114,\n",
      "           117, 10880,   113,  3453,   110,   114,   117,  1105, 12477,   108,\n",
      "           108,  2495,  1182,   108,   108, 14516,   113,  4650,   119,   129,\n",
      "           110,   114,  1127,  1103,  1211,  1887,  8006,   117,  1105,   172,\n",
      "           108,   108,  1231,   108,   108,  7172,   108,   108,  1120,  5266,\n",
      "           113,  2539,   110,   114,   117,  1105,   192,   108,   108,  1119,\n",
      "          1162,   108,   108,   195,  1279,   113,  1969,   110,   114,  1127,\n",
      "          1103,  1211,  1887,  5300,   119,  1211,  4420,  1114, 20673,  1125,\n",
      "           172,   108,   108,  1231,   108,   108,  7172,   108,   108,  1120,\n",
      "          5266,   113,  5899,   119,   123,   110,   114,  1133,  1178,  1512,\n",
      "           110,  1125,  9304,   108,   108,  1113,   108,   108, 22572,  1465,\n",
      "           108,   108,   181,  4943,   119, 13280,   108,   108,   182,  1358,\n",
      "           108,   108,  1185,   108,   108,  3254,   108,   108,   185,   108,\n",
      "           108,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"trec_covid_preprocessed_full.pkl\", \"rb\") as f:\n",
    "    processed_records = pickle.load(f)\n",
    "\n",
    "print(type(processed_records))         # What kind of object is it?\n",
    "print(len(processed_records))          # How many items? (if it's a list, dict, etc.)       # If it's a dict\n",
    "print(processed_records[0])            # First item (if it's a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5dd341c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class BioBERTReranker:\n",
    "    def __init__(self, precomputed_embeddings=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        self.model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\").to(device)\n",
    "        self.model.eval()  \n",
    "        \n",
    "        self.doc_embeddings = precomputed_embeddings or {}\n",
    "        \n",
    "        self.query_cache = defaultdict(lambda: None)\n",
    "\n",
    "    def _encode_batch(self, texts, batch_size=32):\n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad(), torch.autocast(device_type=device.type):\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    def precompute_document_embeddings(self, documents, batch_size=32):\n",
    "        doc_embeddings = {}\n",
    "        for doc_id, chunks in tqdm(documents.items(), desc=\"Precomputing docs\"):\n",
    "            if chunks:  \n",
    "                chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "                doc_embeddings[doc_id] = self._encode_batch(chunk_texts, batch_size)\n",
    "        self.doc_embeddings = doc_embeddings\n",
    "        return doc_embeddings\n",
    "\n",
    "    def encode_query(self, query_text, batch_size=16):\n",
    "        if self.query_cache[query_text] is None:\n",
    "            self.query_cache[query_text] = self._encode_batch([query_text], batch_size)[0]\n",
    "        return self.query_cache[query_text]\n",
    "\n",
    "    def rerank(self, query_text, doc_ids, top_k=10, chunk_weight=0.9):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        query_embedding = self.encode_query(query_text)\n",
    "        \n",
    "        doc_similarities = []\n",
    "        for doc_id in doc_ids:\n",
    "            if doc_id not in self.doc_embeddings:\n",
    "                continue\n",
    "                \n",
    "            chunk_embeddings = self.doc_embeddings[doc_id]\n",
    "            if len(chunk_embeddings) == 0:\n",
    "                continue\n",
    "            \n",
    "            similarities = cosine_similarity(\n",
    "                [query_embedding],\n",
    "                chunk_embeddings\n",
    "            )[0]\n",
    "            \n",
    "            max_sim = np.max(similarities)\n",
    "            avg_sim = np.mean(similarities)\n",
    "            final_score = (chunk_weight * max_sim) + ((1 - chunk_weight) * avg_sim)\n",
    "            \n",
    "            doc_similarities.append((doc_id, final_score))\n",
    "        \n",
    "        doc_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"Reranked {len(doc_similarities)} docs in {time.time()-start_time:.2f}s\")\n",
    "        return doc_similarities[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebe1106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00, 28.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked 0 docs in 0.04s\n",
      "Top results: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    reranker = BioBERTReranker()\n",
    "\n",
    "    sample_query = \"COVID-19 transmission in children\"\n",
    "    candidate_doc_ids = [\"8qnrcgnk\", \"785vg6d\", \"ejv2xln0\"]  \n",
    "\n",
    "    reranked = reranker.rerank(sample_query, candidate_doc_ids)\n",
    "    print(\"Top results:\", reranked[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf4a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

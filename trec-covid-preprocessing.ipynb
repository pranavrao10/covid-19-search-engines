{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4f5c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import re \n",
    "from transformers import AutoTokenizer\n",
    "import hashlib\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3efef",
   "metadata": {},
   "source": [
    "##### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b78b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load('cord19/fulltext/trec-covid')\n",
    "docs_iter = dataset.docs_iter()\n",
    "bio_tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc983b92",
   "metadata": {},
   "source": [
    "##### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b33befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_doc_text(doc):\n",
    "    title_str = doc.title or \"\"\n",
    "    abstract_str = doc.abstract or \"\"\n",
    "    body_str = \"\"\n",
    "    if doc.body:\n",
    "        if isinstance(doc.body, list):\n",
    "            sections = [section.text for section in doc.body if section.text]\n",
    "            body_str = \" \".join(sections)\n",
    "        else:\n",
    "            body_str = doc.body.text if getattr(doc.body, 'text', None) else \"\"\n",
    "    full_text = \" \".join([title_str, abstract_str, body_str])\n",
    "    return full_text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\[[0-9]{1,3}\\]', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)         \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    text = text.lower()          \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02e26e",
   "metadata": {},
   "source": [
    "##### Chunk Long Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f674ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_for_biobert(text: str, tokenizer, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        chunk_tokens = tokens[start : start + max_length - 2]\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            chunk_tokens,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True,\n",
    "            is_split_into_words=True,\n",
    "        )\n",
    "        chunks.append(encoded)\n",
    "        start += (max_length - 2)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff5e36",
   "metadata": {},
   "source": [
    "##### Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a41e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_text(text_hash_set, text):\n",
    "    text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    if text_hash in text_hash_set:\n",
    "        return False \n",
    "    else:\n",
    "        text_hash_set.add(text_hash)\n",
    "        return True  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3499ac",
   "metadata": {},
   "source": [
    "##### Main Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef62dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, tokenizer, apply_dedup=False):\n",
    "  \n",
    "    text_hash_set = set()  \n",
    "    records = []\n",
    "\n",
    "    for doc in dataset.docs_iter():\n",
    "        raw_text = combine_doc_text(doc)\n",
    "        if not raw_text:\n",
    "            continue\n",
    "\n",
    "        cleaned = clean_text(raw_text)\n",
    "\n",
    "        if apply_dedup:\n",
    "            if not deduplicate_text(text_hash_set, cleaned):\n",
    "                continue\n",
    "        \n",
    "        biobert_chunks = chunk_for_biobert(cleaned, tokenizer, max_length=512)\n",
    "\n",
    "        records.append({\n",
    "            'doc_id': doc.doc_id,\n",
    "            'cleaned_text': cleaned,\n",
    "            'biobert_encoded_chunks': biobert_chunks\n",
    "        })\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b06269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "processed_records = preprocess_dataset(dataset, bio_tokenizer,  \n",
    "                                       apply_dedup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa22bb",
   "metadata": {},
   "source": [
    "##### Dataset Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "800daa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_for_classical_ir = [\n",
    "    {\n",
    "        \"doc_id\": rec[\"doc_id\"],\n",
    "        \"cleaned_text\": rec[\"cleaned_text\"]\n",
    "    }\n",
    "    for rec in processed_records\n",
    "]\n",
    "with open(\"trec_covid_preprocessed_minimal.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records_for_classical_ir, f, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"trec_covid_preprocessed_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(processed_records, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b5e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
